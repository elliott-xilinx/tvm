{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "import os\n",
    "from ipywidgets import interact\n",
    "\n",
    "import tvm\n",
    "import nnvm\n",
    "import nnvm.symbol as sb\n",
    "import tvm.relay as relay\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# DEBUG\n",
    "import messages\n",
    "messages.DEBUG(False)\n",
    "import pdb\n",
    "os.environ['XDNN_VERBOSE'] = \"1\"\n",
    "\n",
    "from xfdnn.tools.compile.bin.xfdnn_compiler_tvm import TVMCompiler\n",
    "\n",
    "# DATA\n",
    "# data_shape   = (1,3,224,224)\n",
    "\n",
    "# TVM compiler\n",
    "\n",
    "config = {\n",
    "    'netcfg': \"work/tvm_compiler.json\",\n",
    "    'weights': \"work/weights_data.h5\",\n",
    "    'quantizecfg': \"work/tvm_quantizer.json\"\n",
    "}\n",
    "\n",
    "tvm_compiler = TVMCompiler(\n",
    "    netcfg=config['netcfg'],\n",
    "    weights=config['weights']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import models_util.model_util as model_util\n",
    "models = model_util.get_models_dict()\n",
    "\n",
    "@interact(MODEL=sorted(models.keys()))\n",
    "def select_model(MODEL):\n",
    "    global framework, model_name, model_path, opt_model_path, data_io,\\\n",
    "        data_shapes, add_output_layers, input_name, input_shape\n",
    "    \n",
    "    \n",
    "    #model_name = MODEL #'TF-GoogLeNet_bvlc_without_lrn' # # #'TF-ResNet50' #\n",
    "    \n",
    "    print(models[MODEL])\n",
    "    framework         = models[MODEL]['framework']\n",
    "    model_name        = models[MODEL]['model']\n",
    "    model_path        = models[MODEL]['model_path']\n",
    "    opt_model_path    = models[MODEL]['weights_path']\n",
    "    data_io           = models[MODEL]['io']\n",
    "    add_output_layers = models[MODEL]['add_output_layers']\n",
    "    \n",
    "    data_inputs       = models[MODEL]['inputs']\n",
    "    data_input_shapes = models[MODEL]['input_shapes']\n",
    "    data_shapes = {}\n",
    "    for inpt, shape in zip(data_inputs, data_input_shapes):\n",
    "        data_shapes[inpt] = shape\n",
    "        \n",
    "    input_name = list(data_shapes.keys())[0]\n",
    "    input_shape = data_shapes[input_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Framework: {}\".format(framework))\n",
    "print(\"Model path: {}\".format(model_path))\n",
    "print(\"Optional model path: {}\".format(opt_model_path))\n",
    "print(\"Shapes: {}\".format(data_shapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xfdnn.tools.io import load_model_from_file\n",
    "\n",
    "frontend = 'NNVM'\n",
    "\n",
    "if frontend == 'NNVM':\n",
    "    compute_graph, params, data_layout = \\\n",
    "        load_model_from_file(frontend, framework)(model_path, \n",
    "                                                  data_shapes, \n",
    "                                                  opt_model_path)\n",
    "    xfgraph = tvm_compiler.from_nnvm(compute_graph, params, shapes=data_shapes, \n",
    "                                     #output_op = \"InceptionV1/Logits/AvgPool_0a_7x7/AvgPool\",\n",
    "                     data_layout=data_layout) #from_nnvm output_op\n",
    "###elif frontend == 'Relay':\n",
    "###    mod, params, data_layout = \\\n",
    "###        load_model_from_file(frontend, framework)(model_path, data_shapes, \n",
    "###                                                  opt_model_path)\n",
    "###    xfgraph = tvm_compiler.from_relay(mod, params, \n",
    "###                                      data_layout=data_layout,\n",
    "###                                      add_output_layers=add_output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfgraph.visualize('tvm_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUANTIZE\n",
    " \n",
    "import xfdnn.tools.io as xfdnn_io\n",
    "from xfdnn.tools.xfgraph.quantization import XfGraphDefaultQuantizer, XfGraphAddScalingQuantizer\n",
    "\n",
    "calibration_directory = '/workspace/MLsuite/notebooks/calibration_directory'\n",
    "img_io_func = xfdnn_io.load_imgs_from_file(data_io, input_shape[2:4], model_name)\n",
    "\n",
    "quantizer = XfGraphDefaultQuantizer(\n",
    "    xfgraph=xfgraph,\n",
    "    quant_file=config[\"quantizecfg\"], \n",
    "    data_layout='NCHW',\n",
    "    data_loading_func=img_io_func,\n",
    "    calibration_directory=calibration_directory,\n",
    "    cal_size=15\n",
    ")\n",
    "quantizer.quantize()\n",
    " \n",
    "xfgraph.save('xfgraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE\n",
    "tvm_compiler.compile(xfgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TESTING) BUILD FOR INTERNAL XDNN EXECUTION\n",
    "# from xfdnn.tools.xfgraph.xfgraph import XfGraph\n",
    "\n",
    "##xfgraph = XfGraph()\n",
    "##xfgraph.load('xfgraph.json', 'xfgraph.h5')\n",
    "## \n",
    "#xfgraph.build(device='sim', quantcfg=config[\"quantizecfg\"])\n",
    "\n",
    "# xfgraph.build(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING THE INPUT\n",
    "# CHOSE AN IMAGE TO RUN, DISPLAY IT FOR REFERENCE\n",
    "import xfdnn.tools.io as xfdnn_io\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "imagenet_val_set = None\n",
    "with open('/workspace/MLsuite/notebooks/imagenet-val/val_map.txt') as f:\n",
    "    imagenet_val_set = [line.strip('\\n').split(' ') for line in f.readlines()]\n",
    "\n",
    "# NEXT TWO VARIABLES NEED TO BE ADJUSTED TO TRY OUT OTHER INPUTS\n",
    "val_images = [\"/workspace/MLsuite/examples/image_classify/sample_images/dog.jpg\"]\n",
    "input_shape = (1,3,224,224) \n",
    "\n",
    "img = cv2.imread(val_images[0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.title(val_images[0])\n",
    "plt.show()\n",
    "\n",
    "batch_array = np.empty(input_shape, dtype=np.float32, order='C')\n",
    "img_paths = val_images\n",
    "\n",
    "img_io_func = xfdnn_io.load_imgs_from_file(data_io, input_shape[2:4], model_name)\n",
    "\n",
    "data = img_io_func(img_paths)\n",
    "batch_array[:] = data\n",
    "print(batch_array.shape)\n",
    "#print(batch_array[0])\n",
    "\n",
    "# numpy print option\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# SIM\n",
    "inputs = {}\n",
    "# TODO only one input so this is working\n",
    "inputs[input_name] = batch_array # Placeholder / data / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TESTING) RUN ON CPU FOR TESTING PURPOSES\n",
    "#res = xfgraph.run(inputs, #['InceptionV1/Logits/SpatialSqueeze'],\n",
    "#                  batch_size=1)\n",
    "\n",
    "#print(res[0].shape)\n",
    "#print(repr(res[0]))\n",
    "#print(np.max(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RECONSTRUCT AND FUSE THE GRAPH FOR XDNN\n",
    "import contrib_xdnn\n",
    "from graph import graph_reconst\n",
    "gidx = compute_graph.index\n",
    "\n",
    "print(\"--debug: start reconstructing the graph\")\n",
    "graph = graph_reconst(config[\"netcfg\"],gidx.nodes, add_output_layers)\n",
    "\n",
    "print(\"--debug: finished reconstructing the graph\")\n",
    "\n",
    "\n",
    "#shapes = nnvm.compiler.graph_util.infer_shape(compute_graph)\n",
    "\n",
    "# SETUP AND COMPILE THE RECONSTRUCTED NNVM GRAPH\n",
    "\n",
    "target, target_host = 'llvm', 'llvm'\n",
    "params_shapes = dict((k, params[k].shape) for k in params)\n",
    "params_dtypes  = dict((k, params[k].dtype) for k in params)\n",
    "input_type = 'float32'\n",
    "#shape_dict = {'Placeholder': res[0].shape}\n",
    "#shape_dict = {'Placeholder': (1,224,224,3)}\n",
    "shape_dict = {}\n",
    "shape_dict[input_name] = input_shape\n",
    "dtype_dict = {}\n",
    "dtype_dict[input_name] = input_type\n",
    "shape_dict.update(params_shapes)\n",
    "dtype_dict.update(params_dtypes)\n",
    "\n",
    "print(shape_dict)\n",
    "graph, lib, params = nnvm.compiler.build(\n",
    "    graph, target, shape_dict, dtype_dict,\n",
    "    params=params, target_host=target_host)\n",
    "\n",
    "\n",
    "print(\"--debug: finished recompiling NNVM graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the graph using TVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE GRAPH USING TVM\n",
    "from tvm.contrib import graph_runtime\n",
    "ctx = tvm.cpu(0)\n",
    "m = graph_runtime.create(graph, lib, ctx)\n",
    "#m.set_input(Placeholder=np.array(res[0]))\n",
    "#m.set_input(Placeholder=(np.transpose(batch_array,(0,2,3,1))))\n",
    "#inpts = {}\n",
    "#inpts[input_name] = np.array(batch_array)\n",
    "m.set_input(**inputs)\n",
    "m.set_input(**params)\n",
    "# RUN\n",
    "m.run()\n",
    "\n",
    "tvm_output = m.get_output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the graph using DLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE LIB / GRAPH DEF / PARAMS\n",
    "import os\n",
    "current_path = os.getcwd()\n",
    "\n",
    "#FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "dlr_model_path = os.path.join(current_path, 'work/' + model_name)\n",
    "lib.export_library(dlr_model_path + '.so')\n",
    "\n",
    "with open(dlr_model_path + '.json', 'w') as fo:\n",
    "    fo.write(graph.json())\n",
    "\n",
    "with open(dlr_model_path + '.params', 'wb') as fo:\n",
    "    fo.write(nnvm.compiler.save_param_dict(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dlr_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlr import DLRModel\n",
    "\n",
    "dlr_model_path_test = os.path.join(current_path, 'work/')\n",
    "\n",
    "device = 'cpu'\n",
    "model = DLRModel(dlr_model_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM PREDICTION\n",
    "import xfdnn.tools.xfgraph.classification as xfdnn_classification\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x - np.max(x, axis=1, keepdims=True)) / np.expand_dims(np.sum(np.exp(x - np.max(x, axis=1, keepdims=True)), axis=1), axis=1)\n",
    "\n",
    "\"\"\"if frontend == 'NNVM' and (framework == 'Tensorflow' \\\n",
    "    and model_name in ['vgg_16', 'vgg_19']) or \\\n",
    "    (framework == 'MXNet' \\\n",
    "    and model_name in ['resnet_v1_18', 'resnet_v1_34', 'resnet_v1_50']):\n",
    "    res[0] = softmax(res[0])\"\"\"\n",
    "\n",
    "\n",
    "# TODO: Make this more automatic: 1000 <-> 1001\n",
    "def predict(tensor):\n",
    "    raw_predictions = tensor\n",
    "    if raw_predictions.shape[1] == 1000:\n",
    "        label_lst = [elem[1] for elem in imagenet_val_set[:raw_predictions.shape[0]]]\n",
    "        synset_words = 'synset_words.txt'\n",
    "    elif raw_predictions.shape[1] == 1001:\n",
    "        # for inception, ...\n",
    "        label_lst = [int(elem[1]) + 1 for elem in imagenet_val_set[:raw_predictions.shape[0]]]\n",
    "        synset_words = 'synset_words_1001.txt'\n",
    "    else:\n",
    "        raise ValueError(\"Unknown number of predicted categories: {}\".format(raw_predictions.shape[1]))\n",
    "    \n",
    "    top_1 = xfdnn_classification.get_top_k_accuracy(raw_predictions, synset_words, 1, label_lst)\n",
    "    top_5 = xfdnn_classification.get_top_k_accuracy(raw_predictions, synset_words, 5, label_lst)   \n",
    "    print(\"Top 1: {}\".format(top_1))\n",
    "    print(\"Top 5: {}\".format(top_5))\n",
    "\n",
    "# predict(res[0])\n",
    "predict(tvm_output.asnumpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
